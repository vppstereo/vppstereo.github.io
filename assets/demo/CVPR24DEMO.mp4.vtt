WEBVTT


1
00:00:00.050 --> 00:00:04.670
Hi, I'm Luca Bartolomei and I'm going to
present you our demo "Robust depth perception

2
00:00:04.670 --> 00:00:06.470
through virtual pattern projection".

3
00:00:06.500 --> 00:00:11.060
Our code, trained models, and paper are
available on our project page.

4
00:00:11.090 --> 00:00:15.830
Given a pair of stereo images, stereo
algorithms try to resolve the so-called

5
00:00:15.830 --> 00:00:17.330
"correspondence problem".

6
00:00:17.360 --> 00:00:22.280
This problem is not always easy: uniform
areas, such as the wall shown in the figure

7
00:00:22.280 --> 00:00:24.080
make the problem ambiguous.

8
00:00:24.110 --> 00:00:28.940
Furthermore, deep stereo networks suffer
when dealing with unseen scenarios.

9
00:00:28.940 --> 00:00:33.560
Active stereo deals with ambiguous regions
using a physical pattern projector, which

10
00:00:33.560 --> 00:00:35.330
aims to ease correspondences.

11
00:00:35.360 --> 00:00:40.220
However, a pattern projector is not feasible
in outdoor scenarios where ambient light

12
00:00:40.220 --> 00:00:42.080
cancels out the projected pattern.

13
00:00:42.110 --> 00:00:47.090
Furthermore, active light decreases
proportionally to the square of the distance:

14
00:00:47.090 --> 00:00:51.380
consequentially, active stereo cannot deal
with long distances.

15
00:00:51.380 --> 00:00:57.530
Inspired by active stereo, our technique,
dubbed VPP, virtually projects a pattern into

16
00:00:57.530 --> 00:01:00.580
stereo image pair according to sparse depth
measurements.

17
00:01:00.890 --> 00:01:06.140
We assume a calibrated setup composed of a
stereo camera and a depth sensor, appropriate

18
00:01:06.140 --> 00:01:07.580
for the final environment.

19
00:01:07.580 --> 00:01:12.500
As shown in the left figure, our proposal
increases dramatically the accuracy of the

20
00:01:12.500 --> 00:01:15.920
stereo network, even with a small percentage
of depth seeds.

21
00:01:15.950 --> 00:01:21.050
Meanwhile, in the right figure, we show that
a LiDAR depth sensor is suitable for outdoor

22
00:01:21.050 --> 00:01:26.750
environments. Our proposal outperforms state
of the art fusion methods even with few depth

23
00:01:26.750 --> 00:01:28.850
points, such as 1% of the whole image.

24
00:01:28.880 --> 00:01:33.800
Finally, as shown in the figure, the virtual
pattern reduces the domain shift issue

25
00:01:33.800 --> 00:01:35.450
without requiring an additional training
procedure.

26
00:01:37.870 --> 00:01:43.630
We resume our proposal with the following
claims: even with few sparse depth seeds, our

27
00:01:43.630 --> 00:01:48.580
virtual patterned paradigm outperforms state
of the art sensor fusion methods; it has a

28
00:01:48.580 --> 00:01:53.920
compelling ability to tackle domain shift
issues; acting before any processing occurs

29
00:01:53.920 --> 00:01:58.030
it can be seamlessly deployed with any
stereo algorithm or deep network without

30
00:01:58.030 --> 00:02:01.810
modifications, and benefit from future
progress in the field.

31
00:02:01.930 --> 00:02:06.880
Moreover, in contrast to active stereo
systems, using a depth sensor in place of a

32
00:02:06.880 --> 00:02:12.070
pattern projector: it can work under
sunlight indoors and outdoors at long and

33
00:02:12.070 --> 00:02:16.780
close ranges, and without any additional
processing cost for the selected stereo

34
00:02:16.780 --> 00:02:21.910
matcher; it is more effective even in the
specific application domain of projector

35
00:02:21.910 --> 00:02:26.410
based systems; it does not require
additional hardware, such as an additional

36
00:02:26.410 --> 00:02:32.290
RGB camera; the virtual projection paradigm
can be tailored on the fly to adapt to the

37
00:02:32.290 --> 00:02:36.970
image content, and is agnostic to dynamic
objects in Egomotion.

38
00:02:36.970 --> 00:02:42.280
As for previous methods, our proposal relies
on sparse depth seeds, but differentially

39
00:02:42.280 --> 00:02:47.020
from them we inject sparse points directly
into images using a virtual pattern.

40
00:02:47.020 --> 00:02:52.150
For each known point, we convert depth into
disparity then we apply a virtual pattern in

41
00:02:52.150 --> 00:02:55.600
each stereo image accordingly to a
patterning strategy.

42
00:02:55.690 --> 00:03:00.760
Our method does not require any change or
assumption in the stereo matcher: we assume

43
00:03:00.760 --> 00:03:05.770
it to be a black box model that requires a
pair of rectified stereo images and produces

44
00:03:05.800 --> 00:03:06.910
a disparity map.

45
00:03:06.910 --> 00:03:13.150
We built a functional prototype composed of
an OAK-D Lite stereo camera with a built in

46
00:03:13.150 --> 00:03:19.990
stereo matching algorithm, and an Intel
RealSense L515 LiDAR as the sparse depth

47
00:03:19.990 --> 00:03:24.670
sensor. We managed to calibrate the setup
using a standard chessboard calibration

48
00:03:24.670 --> 00:03:31.240
algorithm between the IR camera of L515 and
the left camera of OAK-D Lite.

49
00:03:31.240 --> 00:03:36.670
We also built a CNC machined aluminum
support to guarantee a stable calibration

50
00:03:36.670 --> 00:03:43.150
over time. L515 and OAK-D Lite share the
same clock as the host machine and are time

51
00:03:43.150 --> 00:03:45.190
synchronized using the nearest timestamp
algorithm.

52
00:03:46.030 --> 00:03:50.470
Kudos to Bachelor student Nicole Ferrari for
the algorithm implementation.

53
00:03:50.950 --> 00:03:56.740
Firstly, the Intel Realsense L515 captures
sparse depth points with high confidence.

54
00:03:57.280 --> 00:04:03.250
The resulting sparse depth map is
reprojected from the L515 reference frame to

55
00:04:03.250 --> 00:04:08.110
the OAK-D left camera, using the rigid
transformation previously found using the

56
00:04:08.110 --> 00:04:10.120
chessboard calibration algorithm.

57
00:04:14.570 --> 00:04:19.610
Finally, the reprojected depth and the
vanilla pair of stereo images are fed into

58
00:04:19.610 --> 00:04:25.490
our VPP module, which produces two enhanced
stereo images in a fraction of a second.

59
00:04:25.520 --> 00:04:31.430
These VPP images are fed again into the
OAK-D stereo algorithm to obtain a robust

60
00:04:31.430 --> 00:04:33.050
disparity estimation.

61
00:04:33.080 --> 00:04:37.880
Our graphical interface permits us to show
the benefits of our proposal in critical

62
00:04:37.880 --> 00:04:42.740
areas, such as textureless regions, with
respect to the vanilla passive stereo.

63
00:04:43.250 --> 00:04:47.570
Let's see in detail a pair of images enhanced
with our virtual pattern projection.

64
00:04:48.550 --> 00:04:53.260
This illustration shows a Middlebury stereo
frame enhanced with our adaptive random

65
00:04:53.260 --> 00:04:55.150
patch-based virtual pattern.

66
00:04:55.690 --> 00:05:00.670
On the upper part of the left image, we can
appreciate our occlusion handling strategy,

67
00:05:00.670 --> 00:05:05.440
and on the top of the right image, the
corresponding area with subpixel splatting.

68
00:05:06.370 --> 00:05:11.080
Furthermore, as visible in the left-bottom
part of the left image, our virtual pattern

69
00:05:11.080 --> 00:05:15.400
is also projected on the left border
occlusion to maintain consistency.

70
00:05:16.360 --> 00:05:21.070
Lastly, our adaptive patches guarantee the
preservation of depth discontinuities and

71
00:05:21.070 --> 00:05:27.100
thin details. We assess the performance of
our framework using unfiltered depth hints

72
00:05:27.100 --> 00:05:33.010
sourced from off the shelf LiDAR sensors
using data sets such as KITTI, DSEC, and

73
00:05:33.010 --> 00:05:38.860
M3ED. As shown in the figure, our framework
can leverage sparse depth points to solve the

74
00:05:38.860 --> 00:05:42.850
textureless region issue, whereas
traditional projected patterns would be

75
00:05:42.850 --> 00:05:48.580
ineffective. To conclude, we embark on a
comparison between the effectiveness of VPP

76
00:05:48.610 --> 00:05:54.370
against the traditional active stereo setup,
typically composed of an IR stereo camera and

77
00:05:54.370 --> 00:05:57.270
an IR projector, using the SIMSTEREO 
dataset.

78
00:05:57.910 --> 00:06:03.100
Some deep networks, such as DLNR, could
often produce artifacts when fed with stereo

79
00:06:03.100 --> 00:06:05.710
pairs acquired with IR pattern projection.

80
00:06:05.710 --> 00:06:10.210
In contrast, the same networks can
seamlessly take advantage of our virtual

81
00:06:10.210 --> 00:06:12.310
patterns. Thanks for the attention.